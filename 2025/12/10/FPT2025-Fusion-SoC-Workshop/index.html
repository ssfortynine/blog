<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> (FPT2025)Fusion SoC Workshop · ssfortynine's Blog</title><meta name="description" content="(FPT2025)Fusion SoC Workshop - ssfortynine"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/cat.png"><link rel="stylesheet" href="/css/apollodark.css"><link rel="search" type="application/opensearchdescription+xml" href="https://ssfortynine.xyz/sitemap.xml/atom.xml" title="ssfortynine's Blog"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="ssfortynine's Blog" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/cat.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/tags" target="_self" class="nav-list-link">TAGS</a></li><li class="nav-list-item"><a href="https://github.com/ssfortynine" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">(FPT2025)Fusion SoC Workshop</h1><div class="post-info">Dec 10, 2025<span class="post-tags"><a href="/tags/Conference/" class="tag-pill"><span class="hash">#</span><span class="tag-name">Conference</span></a></span></div><div class="post-content"><h2 id="The-Front-End-Tools"><a href="#The-Front-End-Tools" class="headerlink" title="The Front-End Tools"></a>The Front-End Tools</h2><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>在异构计算领域，传统的编译器流程（如 GCC&#x2F;Clang）主要负责将高级语言转换为线性的机器指令流。</p>
<p>然而，对于 CGRA 这种空间架构而言，编译器面临着完全不同的挑战：它不仅需要生成指令，更需要从源码中提取<strong>数据流图</strong>（DFG）或<strong>控制数据流图</strong>（CDFG），以便映射到二维的处理单元（PE）阵列上<br><img src="/assets/Pastedimage20251209200435.png" alt="alt text"><br><img src="/assets/Pastedimage20251209200450.png" alt="alt text"><br>传统编译流程与 CGRA 编译流程的对比，后者需要生成 DFG&#x2F;CDFG 并进行空间映射</p>
<p><strong>Fusion CGRA SoC</strong> 是一个典型的异构系统，包含 RISC-V 主控核、DMA 控制器、多库 Scratchpad Memory (SPM) 以及核心的 CGRA 阵列。为了充分利用这一硬件的性能，需要解决三大问题：</p>
<ol>
<li><strong>控制流的复杂性</strong>：如何将高级语言中的嵌套循环和条件分支高效映射到硬件？</li>
<li><strong>算子的专用性</strong>：如何自动提取并利用硬件提供的特殊功能单元（如自动地址生成、累加器）？</li>
<li><strong>优化的全局性</strong>：如何从单纯的内核加速扩展到整个 SoC 的系统级流水线优化？<br><img src="/assets/Pastedimage20251209200654.png" alt="alt text"><br>Fusion SoC 的硬件架构概览，展示了 RISC-V Host 与 CGRA Array 的交互关系</li>
</ol>
<h3 id="第一阶段：CO-Compiler-——-基于-LLVM-的通用性突破"><a href="#第一阶段：CO-Compiler-——-基于-LLVM-的通用性突破" class="headerlink" title="第一阶段：CO-Compiler —— 基于 LLVM 的通用性突破"></a>第一阶段：CO-Compiler —— 基于 LLVM 的通用性突破</h3><p><strong>CO-Compiler (ASP-DAC 2024)</strong> 选择构建在成熟的 LLVM 生态之上。LLVM 的中间表示（IR）虽然通过静态单赋值（SSA）形式很好地表达了数据依赖，但其控制流表示（如 <code>Phi</code> 节点）并不直接兼容 CGRA 的空间映射。<br><img src="/assets/Pastedimage20251209201310.png" alt="alt text"></p>
<h4 id="从-LLVM-IR-到-CDFG：控制流的硬件化重构"><a href="#从-LLVM-IR-到-CDFG：控制流的硬件化重构" class="headerlink" title="从 LLVM IR 到 CDFG：控制流的硬件化重构"></a>从 LLVM IR 到 CDFG：控制流的硬件化重构</h4><p>LLVM IR 使用 <code>br</code>（分支跳转）和 <code>Phi</code> 指令来管理控制流，这在冯·诺依曼架构上是最高效的。但在 CGRA 上，我们需要将这些控制流转换为数据通路上的选择逻辑。</p>
<p>CO-Compiler 引入了一套基于<strong>支配树</strong>（Dominator Tree）的分析算法。编译器遍历基本块（Basic Block），分析数据在不同路径下的汇聚情况，将软件层面的 <code>Phi</code> 指令转换为硬件可执行的 <strong>SELECT（多路选择器）节点</strong>。<br><img src="/assets/Pastedimage20251209201347.png" alt="alt text"><br><img src="/assets/Pastedimage20251209201811.png" alt="alt text"><br>该图展示如何从 LLVM 的 Basic Blocks 和 Phi 节点推导出带有 SELECT 节点的 CDFG<br>更进一步，为了处理循环中的状态更新，将 Phi 节点细分为两类：</p>
<ul>
<li><strong>General Phi</strong>：用于普通的<code> if-else</code> 条件选择，映射为由条件信号控制的 MUX。</li>
<li><strong>Cumulative Phi</strong>：用于循环迭代中变量的自我更新（如 <code> i++</code> 或 <code>sum += a[i]</code>）。这类 <code>Phi</code> 节点依赖于回边（Back-edge），在硬件上被转换为由 <code>LoopStart</code> 信号控制的初始值选择逻辑，从而支持任意层级的嵌套循环</li>
</ul>
<h4 id="GEP-节点解析与线性访存模式提取"><a href="#GEP-节点解析与线性访存模式提取" class="headerlink" title="GEP 节点解析与线性访存模式提取"></a>GEP 节点解析与线性访存模式提取</h4><h5 id="1-基于-LLVM-IR-的-GEP-指令解析"><a href="#1-基于-LLVM-IR-的-GEP-指令解析" class="headerlink" title="1. 基于 LLVM IR 的 GEP 指令解析"></a>1. 基于 LLVM IR 的 GEP 指令解析</h5><p>在 LLVM IR 中，数组访问通常通过 <code>GetElementPtr</code> (GEP) 指令完成。如果直接将 GEP 指令映射为 PE 上的加法和乘法运算，将会消耗大量的计算资源。GEP 指令通过基地址（Base Pointer）和一系列索引（Indices）来计算目标元素的内存地址。</p>
<p>CO-Compiler 的前端分析 Pass 首先扫描循环体内的所有 GEP 指令，解析其操作数结构。根据 C&#x2F;C++ 的多维数组内存布局，一个典型的地址计算公式可以表示为<br><img src="/assets/Pastedimage20251209203518.png" alt="alt text"><br>其中，N 为数组维度，DimScale_i 为第 i 维的跨度（由数组类型决定），Index_i 为该维度的下标。</p>
<p>CO-Compiler 实现了一套<strong>线性访存分析</strong>（Linear Memory Access Analysis）机制。它检查 GEP 指令是否满足关于循环归纳变量的仿射变换关系。</p>
<p><img src="/assets/Pastedimage20251209202207.png" alt="alt text"><br>该图展示编译器对 LLVM IR 中的 <code>getelementptr</code> 指令进行解析，识别基地址、数组维度形状（Array Shape）以及各维度的偏移量。</p>
<h5 id="2-仿射变换与线性性验证"><a href="#2-仿射变换与线性性验证" class="headerlink" title="2. 仿射变换与线性性验证"></a>2. 仿射变换与线性性验证</h5><p>为了确定该访存操作是否能被硬件专用的地址生成单元（AGU）支持，编译器需要验证地址表达式是否关于循环归纳变量（Induction Variable）构成<strong>仿射变换（Affine Transformation）</strong>。<br>判定为“线性访存”需满足以下三个严格条件：</p>
<ol>
<li><strong>计算封闭性</strong>：地址表达式完全由常数、循环不变量（Loop-invariant）和循环归纳变量构成。</li>
<li><strong>线性步进</strong>：循环归纳变量在迭代过程中呈线性变化（如<code> i++</code> 或 <code>i += c</code>）。</li>
<li><strong>仿射映射</strong>：从归纳变量到最终内存地址的映射关系是线性的。<br>当满足上述条件时，复杂的地址计算逻辑可以被简化为一个线性方程：<br><img src="/assets/Pastedimage20251209204735.png" alt="alt text"><br>其中 $M$ 是循环嵌套层级，$Coeff_j$ 是第 $j$ 层循环变量 $Var_j$ 对应的线性系数<br><img src="/assets/Pastedimage20251209204949.png" alt="alt text"><br>该图展示多层嵌套循环中，循环变量与最终物理地址之间的数学映射关系</li>
</ol>
<h5 id="3-硬件参数提取与-AGU-配置"><a href="#3-硬件参数提取与-AGU-配置" class="headerlink" title="3. 硬件参数提取与 AGU 配置"></a>3. 硬件参数提取与 AGU 配置</h5><p>一旦确认为线性访存模式，CO-Compiler 将不再生成用于计算地址的计算节点（Add&#x2F;Mul Nodes），而是直接提取描述该访存模式的三组关键参数，用于配置 CGRA 的输入&#x2F;输出单元（IO Unit）或地址生成单元：</p>
<ul>
<li><strong>起始地址（Start Address）</strong>：由基地址和归纳变量的初始值确定的首个访问地址。</li>
<li><strong>步长（Stride）</strong>：对应每一层循环迭代时，物理地址的跳变值。对于多层嵌套循环，编译器会提取出一组 Stride 值（如 Stride1, Stride2…），分别对应内层和外层循环。</li>
<li><strong>计数值（Count）</strong>：对应每一层循环的迭代次数。<br><img src="/assets/Pastedimage20251209205141.png" alt="alt text"></li>
</ul>
<h5 id="4-资源优化的收益与回退机制"><a href="#4-资源优化的收益与回退机制" class="headerlink" title="4. 资源优化的收益与回退机制"></a>4. 资源优化的收益与回退机制</h5><p>通过上述转换，原本需要由多个 PE 级联完成的乘加树（Mul-Add Tree）运算，被转化为静态的配置参数。</p>
<ul>
<li><strong>资源释放</strong>：实验表明，这一优化显著减少了 PE 的占用率，使得更多 PE 可用于核心的数据处理（如卷积、矩阵乘法），从而提升了算力的有效利用率。</li>
<li><strong>回退机制</strong>：对于无法通过线性分析的<strong>不规则访存</strong>（Irregular Memory Access），例如间接寻址 <code>A[B[i]]</code> 或非线性下标 <code> A[i*i]</code>，编译器会自动回退到保守策略，生成完整的乘加运算数据流图，并映射到 PE 阵列执行，以确保程序的正确性。<br><img src="/assets/Pastedimage20251209205329.png" alt="alt text"><br>对比图，左侧是未优化前占用大量 PE 的地址计算树，右侧是优化后仅需配置参数的 Load 节点</li>
</ul>
<h4 id="专用算子提取"><a href="#专用算子提取" class="headerlink" title="专用算子提取"></a>专用算子提取</h4><p>为了进一步降低 Initiation Interval (II)，CO-Compiler 能够自动识别高级语言中的特定模式并映射到专用硬件算子：</p>
<ul>
<li><strong>累加器序列（Accumulation Series）</strong>：识别带有反馈路径的加法链，映射为硬件累加器。如图所示将通用的加法循环逻辑转换为专用 Accumulator 硬件算子的过程。<br><img src="/assets/Pastedimage20251209205604.png" alt="alt text"></li>
<li><strong>条件访存（Conditional Memory Access）</strong>：将受控的 Load&#x2F;Store 转换为 CLOAD&#x2F;CSTORE，利用 Predicate 信号直接控制内存读写使能，避免了复杂的控制流分歧。如图展示如何将 if-else 控制下的内存访问转换为带使能端的 CStore&#x2F;CLoad 操作。<br><img src="/assets/Pastedimage20251209205646.png" alt="alt text"></li>
</ul>
<h3 id="第二阶段：Adora-Compiler-——-基于-MLIR-的端到端全栈优化"><a href="#第二阶段：Adora-Compiler-——-基于-MLIR-的端到端全栈优化" class="headerlink" title="第二阶段：Adora Compiler —— 基于 MLIR 的端到端全栈优化"></a>第二阶段：Adora Compiler —— 基于 MLIR 的端到端全栈优化</h3><p>随着 AI 和张量计算需求的增加，LLVM 底层 IR 在进行高层循环变换（如 Tiling, Fusion）时显得力不从心。为此，开发了 <strong>Adora Compiler (DAC 2025)</strong>，利用 <strong>MLIR</strong> 的多层抽象能力，实现了从算子级到系统级的全栈优化。这是MLIR 工具链流程<br><img src="/assets/Pastedimage20251209210047.png" alt="alt text">该图对比 LLVM 与 MLIR 在抽象层级、可扩展性及优化能力上的差异。<br><img src="/assets/Pastedimage20251209205816.png" alt="alt text"></p>
<h4 id="基于多面体模型的数据流层级优化"><a href="#基于多面体模型的数据流层级优化" class="headerlink" title="基于多面体模型的数据流层级优化"></a>基于多面体模型的数据流层级优化</h4><p>Adora 充分利用 MLIR 的 <strong>Affine Dialect</strong>，在多面体模型（Polyhedral Model）的理论框架下进行激进的循环变换。</p>
<ul>
<li><strong>智能循环分块（Loop Tiling）</strong>：针对 CGRA 有限的 SPM 容量，Adora 自动计算最优分块大小，确保数据局部性，大幅减少 DRAM 访问。</li>
<li><strong>展开与压缩（Unroll-and-Jam）</strong>：为了提升并行度，Adora 对循环进行展开。更重要的是，它通过依赖多面体分析（Dependency Polyhedron）检测潜在的存储体冲突（Bank Conflict），智能选择展开策略。</li>
<li><strong>循环重排（Loop Reordering）</strong>：自动搜索最优的循环嵌套顺序，最大化数据复用。<br>该图展示经过 MLIR 优化后生成的复杂 CDFG 实例（以 MatMul 为例）。<br><img src="/assets/Pastedimage20251209210302.png" alt="alt text"></li>
</ul>
<h4 id="任务流层级优化：超越内核"><a href="#任务流层级优化：超越内核" class="headerlink" title="任务流层级优化：超越内核"></a>任务流层级优化：超越内核</h4><p>Adora 还通过<strong>任务流优化</strong>（Task-Flow Optimization）来提升整个 SoC 的运行效率。</p>
<ul>
<li><strong>软件流水线与乒乓缓存（Ping-Pong Buffering）</strong>：编译器生成双缓冲机制，利用 SoC 的乱序执行能力，在计算当前数据块的同时，通过 DMA 预取下一块数据。这种细粒度的流水线设计有效掩盖了数据传输延迟。</li>
<li><strong>指令去重</strong>：通过分析任务间的依赖关系，Adora 能够剔除冗余的配置指令和数据搬运请求。</li>
</ul>
<h4 id="AI-部署流程与自动化探索"><a href="#AI-部署流程与自动化探索" class="headerlink" title="AI 部署流程与自动化探索"></a>AI 部署流程与自动化探索</h4><p>Adora 提供了从 PyTorch&#x2F;ONNX 到 CGRA 的完整自动化路径。对于循环变换带来的巨大设计空间（分块大小 x 展开因子 x 重排顺序），Adora 摒弃了传统的遗传算法，设计了一套基于<strong>帕累托最优</strong>（Pareto-optimal）的快速搜索算法，以资源利用率和通信量为双重指标，实现了 6 倍以上的搜索速度提升。<br>该图展示 ResNet18 的层级拆解、计算核提取以及在 CGRA 上的流水线执行过程。<br><img src="/assets/Pastedimage20251209211335.png" alt="alt text"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><strong>LLVM 工具链</strong>证明了 CGRA 可以像通用处理器一样，处理复杂的 C&#x2F;C++ 嵌套循环与不规则控制流，为通用计算卸载提供了可能。</li>
<li><strong>MLIR 工具链</strong>则展示了在 AI 与张量计算领域，通过高层语义分析与系统级流水线编排，软硬件协同设计可以达到媲美甚至超越 FPGA 的能效比。<br>该图详细对比总结两个工具链的优势、共同点与适用场景<br><img src="/assets/Pastedimage20251209211503.png" alt="alt text"></li>
</ul>
<h2 id="Architecture-and-Software-Optimizations"><a href="#Architecture-and-Software-Optimizations" class="headerlink" title="Architecture and Software Optimizations"></a>Architecture and Software Optimizations</h2><h3 id="CGRA-的核心痛点"><a href="#CGRA-的核心痛点" class="headerlink" title="CGRA 的核心痛点"></a>CGRA 的核心痛点</h3><p>传统的 CGRA 主要由许多的粗粒度的 PE (处理单元)组成，每个 PE 里有一个 ALU（算数逻辑单元）做加减乘除。它们比较适合做规整的矩阵运算。</p>
<p><img src="/assets/IMG_0033.jpeg" alt="alt text"></p>
<p>但是在实际的程序中，很多循环是不规则的，包含：</p>
<ul>
<li><ol>
<li>动态边界循环：循环次数编译时不知道 (例如 <code>while(i&lt;N)</code>，N 时变量)</li>
</ol>
</li>
<li><ol start="2">
<li><code>if-then-else(ITE)</code>：循环体中有复杂的条件分支</li>
</ol>
</li>
<li>复杂的内存依赖：比如 <code>A[B[i]]</code> 这种间接寻址</li>
</ul>
<div class="tip">
问题在于处理这些不规则控制流 （如计算 if 条件、跳转信号）需要大量的 `1-bit` 位运算 (`AND,OR,NOT`)。如果用一个宽 32-bit 的 ALU 去算一个 1-bit 的结果，不仅浪费资源，而且产生大量的控制信号连线，导致性能和能效的暴跌
</div>

<div class="tip">
对于传统的 CGRA 并不是很熟悉，但是在设计的时候就应该考虑控制流的问题，保证在 PE 单元计算的时候只是计算矩阵，这样不会出现上述的问题，一些加速器以及 NPU 都是这么设计
</div>

<h3 id="COFFA-解决方案：软硬兼施"><a href="#COFFA-解决方案：软硬兼施" class="headerlink" title="COFFA 解决方案：软硬兼施"></a>COFFA 解决方案：软硬兼施</h3><p>COFFA 提出了“Fused-Grained”（融合粒度）的概念</p>
<h4 id="硬件架构：FGRA"><a href="#硬件架构：FGRA" class="headerlink" title="硬件架构：FGRA"></a>硬件架构：FGRA</h4><p>COFFA 设计了一种特殊的加速器，集成在 RISC-V SoC 中</p>
<p><img src="/assets/IMG_0035.jpeg" alt="alt text"></p>
<ul>
<li>FPE (Fused-Grained PE): <ul>
<li>传统部分：包含一个 ALU (做加法、乘法)</li>
<li>创新部分：嵌入了一个 LUT </li>
<li>融合设计：ALU 和 LUT 之间有本地直连线。LUT 专门处理负责 <code>if-else</code> 的条件判断和控制逻辑，ALU 负责真正的数据计算，这样既不浪费 ALU ，又能快速处理控制流</li>
</ul>
</li>
</ul>
<p><img src="/assets/IMG_0034.jpeg" alt="alt text"><br>图中展示了处理 <code>if-else</code> 的四种传统方式（a-d）和 COFFA 方法 (e)，虽然 Partial Predication 并行度高，但是需要额外的选择节点 (Select Node)，而 COFFA 把“计算条件”的逻辑 (LUT)和“选择结果”的逻辑 (Mux&#x2F;ALU)融合到一个 PE 里了，只需要一个周期就能出结果，并不需要其他方法那样等。</p>
<ul>
<li>FGIB：互联网络也能同时传输粗粒度数据和细粒度控制信号</li>
<li>IOB: 智能的存储控制器，支持简单的线性访问模式，也支持由 LUT 控制的复杂访问模式<br><img src="/assets/IMG_0037.jpeg" alt="alt text"></li>
</ul>
<h4 id="软件编译器：业界首个引入布尔代数优化的-CGRA-编译器"><a href="#软件编译器：业界首个引入布尔代数优化的-CGRA-编译器" class="headerlink" title="软件编译器：业界首个引入布尔代数优化的 CGRA 编译器"></a>软件编译器：业界首个引入布尔代数优化的 CGRA 编译器</h4><p>这是 COFFA 的优点，以前的编译器只管把运算映射到 PE 上，不管逻辑优化<br><img src="/assets/IMG_0039.jpeg" alt="alt text"></p>
<ul>
<li>前端（LLVM-based）:<ul>
<li>把 <code>if-else</code> 分支转换成数据选择操作 (partial predication)</li>
<li>识别出哪些是控制流逻辑，（比如 <code>condition = (a &gt; b) &amp;&amp; (c &lt; d)</code>）</li>
</ul>
</li>
<li>后端优化<ul>
<li>引入 Yosys: COFFA 把提取出来的控制逻辑喂给 yosys</li>
<li>布尔优化：yosys 会自动化简逻辑门（比如把多层逻辑合并），然后映射到 FPE 里的 LUT 上。这大大减少了需要的 PE 数量和连线长度</li>
<li>映射 (Mapping): 使用模拟退火算法，将优化后的计算图（CDFG）映射到硬件阵列上</li>
</ul>
</li>
</ul>
<div class="tip">
在参加完 EDA 比赛完之后，老师也是想要提出将起前端设计与后端优化结合起来，实现一个前端设计+专用于设计优化的 eda 算法->实现更好的硬件电路效果，<mark style="background: #ADCCFFA6;">属于软硬件协同优化中的编译器-架构协同设计范畴，通过使用“跨层优化”的方式实现性能提升</mark>

<ul>
<li>传统分层：算法-&gt;高级语言-&gt;编译器-&gt;指令集-&gt;微架构-&gt;电路</li>
</ul>
<p>这应该算是这种优化算法的实例，这部分可以做的还有很多</p>
</div>

<h3 id="代码阅读"><a href="#代码阅读" class="headerlink" title="代码阅读"></a>代码阅读</h3><h4 id="硬件实现"><a href="#硬件实现" class="headerlink" title="硬件实现"></a>硬件实现</h4><ul>
<li>FPE 代码逻辑：<ul>
<li>在代码中的 FPE 类。实例化了 ALU 和 LUT 模块</li>
<li>ALU 的输入不仅可以来自邻居 PE，还可以直接来自内部的 LUT 输出（用于控制信号）。ALU 的比较结果 (如 <code>GreatThan</code>)也也可以直接喂给 LUT。这种“内部短路”设计大大减少了全局布线压力</li>
</ul>
</li>
<li>SoC 集成：<ul>
<li>代码利用 <code>Chipyard</code> 框架，将 FGRA 挂载到 RISC-V (Rocket Core)总线上（TileLink）。这在代码称为 <code>LazyModule</code> 的配置，使得 CPU 可以通过简单的指令配置 FGRA<br><img src="/assets/IMG_0036.jpeg" alt="alt text"></li>
</ul>
</li>
</ul>
<h4 id="编译器实现"><a href="#编译器实现" class="headerlink" title="编译器实现"></a>编译器实现</h4><ul>
<li>前端 Pass<ul>
<li>代码会遍历 LLVM IR。当遇到 <code>Phi</code> 节点（LLVM 中表示分支汇合的节点）时，编译器会将起转换为 <code>Select</code> 节点</li>
<li>动态边界处理：这是一个难点。代中会有专门的 Pass 去分析循环索引 (Induction Variables)。如果是动态边界，它会插入预计算指令（Pre-generation logic），提前算出下一层循环是否结束，从而避免流水线停顿</li>
</ul>
</li>
<li>后端映射与 Yosys 接口<ul>
<li>代码中会有一个步骤将提取出的子图 (Subgraph)转换为 verilog 或 BLIF 格式</li>
<li>调用 yosys 库进行逻辑化简</li>
<li>Mapper: 这是一个图匹配算法，使用模拟退火来寻找将 CDFG 节点放置到 6 x 6 或 8 x 8 阵列上最优解。代码会尽量把 LUT 节点和依赖它的 ALU 节点放在同一个 FPE 或相邻 FPE 中</li>
</ul>
</li>
</ul>
<h2 id="Fusion-DSE-flow"><a href="#Fusion-DSE-flow" class="headerlink" title="Fusion DSE flow"></a>Fusion DSE flow</h2><h3 id="为什么-Fusion-需要-DSE？"><a href="#为什么-Fusion-需要-DSE？" class="headerlink" title="为什么 Fusion 需要 DSE？"></a>为什么 Fusion 需要 DSE？</h3><p>Fusion SoC 架构（如下图所示）包含 Rocket Core、FGRA 阵列、多库 Scratchpad 存储器等多个组件。</p>
<p>为了适应不同的应用场景，设计者需要调整大量的硬件参数，例如：</p>
<ul>
<li><strong>架构层级：</strong> CGRA 的行&#x2F;列数、数据位宽。</li>
<li><strong>存储层级：</strong> Scratchpad 的深度、Buffer 大小。</li>
<li><strong>互连层级：</strong> 输入&#x2F;输出端口数量、路由连接方式。</li>
</ul>
<p>这些参数组合构成了一个<strong>极其庞大且复杂的设计空间</strong>。每一个参数组合（Input）都需要经过耗时的综合与仿真（Expensive Experiment）才能得到面积和吞吐量等指标（Objective Function）。</p>
<p><strong>核心痛点：</strong></p>
<ul>
<li><strong>穷举法（Exhaustive Search）：</strong> 不可能完成，空间太大。</li>
<li><strong>试错法（Trial and Error）：</strong> 效率太低，极其依赖人工经验。</li>
</ul>
<p>因此，我们需要一种智能算法，用最少的实验次数，找到性能最好的配置。</p>
<h3 id="贝叶斯优化（Bayesian-Optimization）"><a href="#贝叶斯优化（Bayesian-Optimization）" class="headerlink" title="贝叶斯优化（Bayesian Optimization）"></a>贝叶斯优化（Bayesian Optimization）</h3><p>解决昂贵黑盒函数优化问题的最佳工具之一是 <strong>贝叶斯优化（BO）</strong>。<br>贝叶斯优化不直接去跑昂贵的仿真，而是建立一个“代理模型”（Surrogate Model）来模拟硬件评估过程。它的工作流程包含三个关键要素：</p>
<ol>
<li><strong>统计模型（Statistical Model）：</strong> 通常使用<strong>高斯过程（Gaussian Process, GP）</strong>。它不仅预测某个配置的性能（均值），还能告诉我们要这一预测的“不确定性”（方差）。</li>
<li><strong>采集函数（Acquisition Function）：</strong> 用来决定“下一步测哪个点”。它需要在<strong>探索</strong>（Exploration，去不确定的地方看看）和<strong>利用</strong>（Exploitation，去已知表现好的地方深挖）之间通过 UCB 或 EI 等策略寻找平衡。</li>
<li><strong>优化器：</strong> 在代理模型上寻找采集函数的最大值。<br>通过这种“预测-采样-更新”的循环，BO 能够快速收敛到全局最优解</li>
</ol>
<h3 id="进阶方案：MoDAF-框架"><a href="#进阶方案：MoDAF-框架" class="headerlink" title="进阶方案：MoDAF 框架"></a>进阶方案：MoDAF 框架</h3><p>虽然传统的 BO 很强大，但在面对 Fusion 这种非凸、多模态的高维设计空间时，仍显吃力。于是提出了一种名为 <strong>MoDAF</strong> 的多目标分治参数调优框架。</p>
<h3 id="MoDAF-的创新点"><a href="#MoDAF-的创新点" class="headerlink" title="MoDAF 的创新点"></a>MoDAF 的创新点</h3><p><strong>1. 动态空间划分（Divide-and-Conquer Strategy）</strong><br>这是该框架的杀手锏。它不再在整个庞大的空间里盲目搜索，而是采用“分而治之”的策略：</p>
<ul>
<li><strong>SVM 划分：</strong> 使用支持向量机（SVM）将设计空间切割为“好”的子空间和“坏”的子空间。</li>
<li><strong>MCTS 搜索：</strong> 利用蒙特卡洛树搜索（MCTS）算法，基于 UCT 指标评估每个子空间的潜力，优先在最有希望的区域（高超体积 Hypervolume）进行采样。</li>
</ul>
<p><strong>2. 混合代理模型（Hybrid Surrogate Modeling）</strong><br>单一的高斯过程模型在处理复杂数据时可能受限。MoDAF 结合了多种模型（如随机森林、深度集成模型等），以提高预测的准确性。</p>
<p><strong>3. 双重采样策略（Dual-Sampling）</strong><br>同时利用 qEHVI（期望超体积改进）和 qUCB 策略，平衡全局搜索与局部挖掘，加速帕累托前沿（Pareto Frontier）的发现。</p>
<h3 id="实验成果"><a href="#实验成果" class="headerlink" title="实验成果"></a>实验成果</h3><p>在与 NSGA-II、MOTPE 等经典算法的对比实验中，MoDAF 展现了显著的优势：</p>
<ul>
<li><strong>收敛速度更快：</strong> 在相同的迭代次数下，MoDAF 能更快地提升超体积（Hypervolume）指标。</li>
<li><strong>解的质量更高：</strong> 能够找到面积更小、吞吐量更高的 Pareto 最优解组合。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>*PPT 以及图示来自 Fusion SoC Workshop：<a target="_blank" rel="noopener" href="https://fpt-2025.lin.pub/#schedule">Fusion SoC: A Fused-Grained Reconfigurable Architecture for Efficient Edge Computing Acceleration</a><br>*开源仓库：<a target="_blank" rel="noopener" href="https://github.com/Dai-dirk/COFFA">GitHub - Dai-dirk&#x2F;COFFA： COFFA：融合粒度可重构架构的共设计框架</a> </p>
</div><div class="post-footer-tags"><a href="/tags/Conference/" class="tag-pill"><span class="hash">#</span><span class="tag-name">Conference</span></a></div></article></div><aside id="toc" class="post-toc-sidebar"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Front-End-Tools"><span class="toc-number">1.</span> <span class="toc-text">The Front-End Tools</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9ACO-Compiler-%E2%80%94%E2%80%94-%E5%9F%BA%E4%BA%8E-LLVM-%E7%9A%84%E9%80%9A%E7%94%A8%E6%80%A7%E7%AA%81%E7%A0%B4"><span class="toc-number">1.2.</span> <span class="toc-text">第一阶段：CO-Compiler —— 基于 LLVM 的通用性突破</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E-LLVM-IR-%E5%88%B0-CDFG%EF%BC%9A%E6%8E%A7%E5%88%B6%E6%B5%81%E7%9A%84%E7%A1%AC%E4%BB%B6%E5%8C%96%E9%87%8D%E6%9E%84"><span class="toc-number">1.2.1.</span> <span class="toc-text">从 LLVM IR 到 CDFG：控制流的硬件化重构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GEP-%E8%8A%82%E7%82%B9%E8%A7%A3%E6%9E%90%E4%B8%8E%E7%BA%BF%E6%80%A7%E8%AE%BF%E5%AD%98%E6%A8%A1%E5%BC%8F%E6%8F%90%E5%8F%96"><span class="toc-number">1.2.2.</span> <span class="toc-text">GEP 节点解析与线性访存模式提取</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%9F%BA%E4%BA%8E-LLVM-IR-%E7%9A%84-GEP-%E6%8C%87%E4%BB%A4%E8%A7%A3%E6%9E%90"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">1. 基于 LLVM IR 的 GEP 指令解析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%80%A7%E9%AA%8C%E8%AF%81"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">2. 仿射变换与线性性验证</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E7%A1%AC%E4%BB%B6%E5%8F%82%E6%95%B0%E6%8F%90%E5%8F%96%E4%B8%8E-AGU-%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">3. 硬件参数提取与 AGU 配置</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E8%B5%84%E6%BA%90%E4%BC%98%E5%8C%96%E7%9A%84%E6%94%B6%E7%9B%8A%E4%B8%8E%E5%9B%9E%E9%80%80%E6%9C%BA%E5%88%B6"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">4. 资源优化的收益与回退机制</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%93%E7%94%A8%E7%AE%97%E5%AD%90%E6%8F%90%E5%8F%96"><span class="toc-number">1.2.3.</span> <span class="toc-text">专用算子提取</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9AAdora-Compiler-%E2%80%94%E2%80%94-%E5%9F%BA%E4%BA%8E-MLIR-%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%85%A8%E6%A0%88%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.</span> <span class="toc-text">第二阶段：Adora Compiler —— 基于 MLIR 的端到端全栈优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E5%B1%82%E7%BA%A7%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.1.</span> <span class="toc-text">基于多面体模型的数据流层级优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E6%B5%81%E5%B1%82%E7%BA%A7%E4%BC%98%E5%8C%96%EF%BC%9A%E8%B6%85%E8%B6%8A%E5%86%85%E6%A0%B8"><span class="toc-number">1.3.2.</span> <span class="toc-text">任务流层级优化：超越内核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AI-%E9%83%A8%E7%BD%B2%E6%B5%81%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E6%8E%A2%E7%B4%A2"><span class="toc-number">1.3.3.</span> <span class="toc-text">AI 部署流程与自动化探索</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Architecture-and-Software-Optimizations"><span class="toc-number">2.</span> <span class="toc-text">Architecture and Software Optimizations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CGRA-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%97%9B%E7%82%B9"><span class="toc-number">2.1.</span> <span class="toc-text">CGRA 的核心痛点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#COFFA-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9A%E8%BD%AF%E7%A1%AC%E5%85%BC%E6%96%BD"><span class="toc-number">2.2.</span> <span class="toc-text">COFFA 解决方案：软硬兼施</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E6%9E%B6%E6%9E%84%EF%BC%9AFGRA"><span class="toc-number">2.2.1.</span> <span class="toc-text">硬件架构：FGRA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AF%E4%BB%B6%E7%BC%96%E8%AF%91%E5%99%A8%EF%BC%9A%E4%B8%9A%E7%95%8C%E9%A6%96%E4%B8%AA%E5%BC%95%E5%85%A5%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0%E4%BC%98%E5%8C%96%E7%9A%84-CGRA-%E7%BC%96%E8%AF%91%E5%99%A8"><span class="toc-number">2.2.2.</span> <span class="toc-text">软件编译器：业界首个引入布尔代数优化的 CGRA 编译器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB"><span class="toc-number">2.3.</span> <span class="toc-text">代码阅读</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.3.1.</span> <span class="toc-text">硬件实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%E5%99%A8%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.3.2.</span> <span class="toc-text">编译器实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fusion-DSE-flow"><span class="toc-number">3.</span> <span class="toc-text">Fusion DSE flow</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-Fusion-%E9%9C%80%E8%A6%81-DSE%EF%BC%9F"><span class="toc-number">3.1.</span> <span class="toc-text">为什么 Fusion 需要 DSE？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%EF%BC%88Bayesian-Optimization%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">贝叶斯优化（Bayesian Optimization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E9%98%B6%E6%96%B9%E6%A1%88%EF%BC%9AMoDAF-%E6%A1%86%E6%9E%B6"><span class="toc-number">3.3.</span> <span class="toc-text">进阶方案：MoDAF 框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MoDAF-%E7%9A%84%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">3.4.</span> <span class="toc-text">MoDAF 的创新点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%88%90%E6%9E%9C"><span class="toc-number">3.5.</span> <span class="toc-text">实验成果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">4.</span> <span class="toc-text">参考资料</span></a></li></ol></aside><a href="#" onclick="toggleTOC();return false;" class="toc-toggle"></a></main><footer><div class="paginator"><a href="/2025/12/10/FPT2025-OpenDRAM/" class="prev">PREV</a><a href="/2025/12/10/FPT2025-AMD-Turtorial/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'ssfortynine seansun';
var disqus_identifier = '2025/12/10/FPT2025-Fusion-SoC-Workshop/';
var disqus_title = '(FPT2025)Fusion SoC Workshop';
var disqus_url = 'https://ssfortynine.xyz/sitemap.xml/2025/12/10/FPT2025-Fusion-SoC-Workshop/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//ssfortynine seansun.disqus.com/count.js" async></script><div class="copyright"><p>© 2025 <a href="https://ssfortynine.xyz/sitemap.xml">ssfortynine</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/ssfortynine/hexo-theme-apollodark" target="_blank">hexo-theme-apollodark</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>